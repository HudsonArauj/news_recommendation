{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scraping poder360 - site de noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date link: https://www.poder360.com.br/2024/09/02\n",
      "Cron job links: 119\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/09/01\n",
      "Cron job links: 56\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/31\n",
      "Cron job links: 76\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/30\n",
      "Cron job links: 142\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/29\n",
      "Cron job links: 117\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/28\n",
      "Cron job links: 149\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/27\n",
      "Cron job links: 148\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/26\n",
      "Cron job links: 128\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/25\n",
      "Cron job links: 61\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/24\n",
      "Cron job links: 83\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/23\n",
      "Cron job links: 84\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/22\n",
      "Cron job links: 115\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/21\n",
      "Cron job links: 118\n",
      "Cron job finished\n",
      "Date link: https://www.poder360.com.br/2024/08/20\n",
      "Cron job links: 112\n",
      "Cron job finished\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_links(url):\n",
    "    # Enviando requisição HTTP para obter o conteúdo da página\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "\n",
    "    # Analisando o conteúdo da página com BeautifulSoup\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Encontrar todas as tags ul que tenham archive-list__list na classe\n",
    "    archive_list = soup.find('ul', class_='archive-list__list')\n",
    "\n",
    "    # Encontrar todas as tags <li> dentro da tag <ul>\n",
    "    li_tags = archive_list.find_all('li')\n",
    "\n",
    "    links = []\n",
    "    # Iterar sobre todas as tags <li> e extrair o href\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        href = a_tag['href']\n",
    "        links.append(href)\n",
    "    return links\n",
    "\n",
    "def extract_article_info(url):\n",
    "    try:\n",
    "        # Fazer a requisição HTTP\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Verificar se a requisição foi bem-sucedida\n",
    "        if response.status_code != 200:\n",
    "            return f\"Erro ao acessar a página: {response.status_code}\"\n",
    "            \n",
    "        # Definir a codificação como UTF-8\n",
    "        response.encoding = 'utf-8'\n",
    "\n",
    "        # Fazer o parsing do HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extrair informações do artigo\n",
    "        if url.startswith('https://www.poder360.com.br/poder-flash'):\n",
    "            title = soup.find('h1', class_='box-poder-flash__title mt-4').text.strip()\n",
    "        else:\n",
    "            title = soup.find('h1', class_='inner-page-section__title title-1').text.strip()\n",
    "        subtitle = soup.find('h2', class_='inner-page-section__line').text.strip()\n",
    "        \n",
    "        # Tentar encontrar a data e o autor\n",
    "        date = soup.find('time')  # Assumindo que a data está dentro de uma tag <time>\n",
    "        if date:\n",
    "            date = date.text.strip()\n",
    "        else:\n",
    "            date = \"Data não encontrada\"\n",
    "        \n",
    "        # Extrair o autor\n",
    "        author_div = soup.find('div', class_='footer-post__box')\n",
    "        if author_div:\n",
    "            author_name = author_div.find('a', class_='author__name')\n",
    "            if author_name:\n",
    "                author = author_name.text.strip()\n",
    "            else:\n",
    "                author = \"Autor não encontrado\"\n",
    "        else:\n",
    "            author = \"Autor não encontrado\"\n",
    "        \n",
    "        # Extrair o texto do artigo\n",
    "        article_body = soup.find('div', class_='inner-page-section__text')\n",
    "        if article_body:\n",
    "            # Encontra e remove o elemento <form> dentro de article_body se ele existir\n",
    "            form = article_body.find('form')\n",
    "            if form:\n",
    "                form.decompose()\n",
    "\n",
    "            # Inicializa uma lista para armazenar o texto dos elementos\n",
    "            text_elements = []\n",
    "\n",
    "            # Itera sobre todos os filhos do article_body\n",
    "            for child in article_body.children:\n",
    "                if child.name == 'p':\n",
    "                    text_elements.append(child.get_text(strip=True))\n",
    "                elif child.name == 'ul':\n",
    "                    # Itera sobre todos os <li> dentro do <ul>\n",
    "                    for li in child.find_all('li'):\n",
    "                        text_elements.append(li.get_text(strip=True))\n",
    "\n",
    "            # Combina todos os textos em uma única string separada por quebras de linha\n",
    "            text = '\\n'.join(text_elements)\n",
    "        else:\n",
    "            text = \"Texto do artigo não encontrado\"\n",
    "        \n",
    "        # Concatenar informações em uma string\n",
    "        article_info = f\"titulo: {title}\\nsubtitulo: {subtitle}\\ndata: {date}\\nautor: {author}\\n\\ntexto: {text}\\n\\n\"\n",
    "        \n",
    "        return article_info\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair informações do artigo: {e}\")\n",
    "        print(f\"URL do artigo: {url}\")\n",
    "\n",
    "def my_scheduled_task(n_dias):\n",
    "    # Calcular a data de ontem\n",
    "    for i in range(1,n_dias):\n",
    "        yesterday = datetime.now() - timedelta(i)\n",
    "        yesterday_str = yesterday.strftime('%Y-%m-%d')\n",
    "\n",
    "        #construir a url das notícias de ontem\n",
    "        url = f'https://www.poder360.com.br/{yesterday.strftime(\"%Y/%m/%d\")}'\n",
    "        print(f\"Date link: {url}\")\n",
    "\n",
    "        links = get_links(url)\n",
    "        links_filtrados = [link for link in links if not link.startswith(\"https://www.poder360.com.br/author/\")]\n",
    "        print(f\"Cron job links: {len(links_filtrados)}\")\n",
    "\n",
    "        news = []\n",
    "        for link in links_filtrados:\n",
    "            article_info = extract_article_info(link)\n",
    "            news.append(article_info)\n",
    "        \n",
    "        # Nome do arquivo\n",
    "        filename = f'news_{yesterday_str}.json'\n",
    "\n",
    "        # Tentar abrir o arquivo e ler seu conteúdo, se existir\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            data = []\n",
    "\n",
    "        # Adicionar a nova notícia à lista\n",
    "        data.append(news)\n",
    "\n",
    "        # Salvar a lista de notícias no arquivo JSON\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "        print(\"Cron job finished\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    my_scheduled_task(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar um arquivo json com todas as noticias\n",
    "\n",
    "for json_file in os.listdir():\n",
    "    if json_file.startswith('news_'):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        try:\n",
    "            with open('news.json', 'r', encoding='utf-8') as f:\n",
    "                all_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            all_data = []\n",
    "        \n",
    "        all_data.extend(data)\n",
    "        \n",
    "        with open('news.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        os.remove(json_file)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ler dados \n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "with open('news.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    \n",
    "flattened_data = [item for sublist in data for item in sublist]\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(flattened_data, columns=[\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(article):\n",
    "    fields = [\"titulo\", \"subtitulo\", \"data\", \"autor\", \"texto\"]\n",
    "    parsed_data = {}\n",
    "\n",
    "    for field in fields:\n",
    "        if f\"{field}:\" in article:\n",
    "            # Encontrar o início do campo\n",
    "            start = article.index(f\"{field}:\") + len(f\"{field}:\")\n",
    "            # Encontrar o fim do campo (ou fim do texto)\n",
    "            end = len(article)\n",
    "            for next_field in fields:\n",
    "                if next_field != field and f\"{next_field}:\" in article[start:]:\n",
    "                    potential_end = article.index(f\"{next_field}:\", start)\n",
    "                    if potential_end < end:\n",
    "                        end = potential_end\n",
    "            # Extrair o valor do campo\n",
    "            parsed_data[field] = article[start:end].strip()\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "# Transformar a lista de artigos em uma lista de dicionários\n",
    "parsed_data = [parse_article(article) for article in flattened_data]\n",
    "\n",
    "# Converter para DataFrame\n",
    "df = pd.DataFrame(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>subtitulo</th>\n",
       "      <th>data</th>\n",
       "      <th>autor</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barroso lança livro sobre Inteligência Artific...</td>\n",
       "      <td>Presidente do STF fez sessão de autógrafos na ...</td>\n",
       "      <td>20.ago.2024 (terça-feira) - 23h28</td>\n",
       "      <td>PODER360</td>\n",
       "      <td>O presidente doSTF(Supremo Tribunal Federal), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marçal apaga 4º vídeo com críticas a Boulos ap...</td>\n",
       "      <td>Segundo o TRE de São Paulo, o conteúdo não tem...</td>\n",
       "      <td>20.ago.2024 (terça-feira) - 23h21</td>\n",
       "      <td>Autor não encontrado</td>\n",
       "      <td>O candidato à Prefeitura de São PauloPablo Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kamala aceita de forma simbólica nomeação do P...</td>\n",
       "      <td>Democrata diz estar “honrada” por receber indi...</td>\n",
       "      <td>20.ago.2024 (terça-feira) - 23h08</td>\n",
       "      <td>PODER360</td>\n",
       "      <td>A vice-presidente dos Estados Unidos,Kamala Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Janja participa da cerimônia de Restauração do...</td>\n",
       "      <td>Primeira-dama foi convidada pelo ministro da J...</td>\n",
       "      <td>20.ago.2024 (terça-feira) - 23h01</td>\n",
       "      <td>PODER360</td>\n",
       "      <td>A primeira-dama, Janja Lula da Silva, particip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump não tem empatia moral, diz ex-secretária...</td>\n",
       "      <td>Ex-apoiadores discursam no 2º dia da Convenção...</td>\n",
       "      <td>20.ago.2024 (terça-feira) - 22h05</td>\n",
       "      <td>PODER360</td>\n",
       "      <td>A ex-secretária de imprensa da Casa Branca do ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titulo  \\\n",
       "0  Barroso lança livro sobre Inteligência Artific...   \n",
       "1  Marçal apaga 4º vídeo com críticas a Boulos ap...   \n",
       "2  Kamala aceita de forma simbólica nomeação do P...   \n",
       "3  Janja participa da cerimônia de Restauração do...   \n",
       "4  Trump não tem empatia moral, diz ex-secretária...   \n",
       "\n",
       "                                           subtitulo  \\\n",
       "0  Presidente do STF fez sessão de autógrafos na ...   \n",
       "1  Segundo o TRE de São Paulo, o conteúdo não tem...   \n",
       "2  Democrata diz estar “honrada” por receber indi...   \n",
       "3  Primeira-dama foi convidada pelo ministro da J...   \n",
       "4  Ex-apoiadores discursam no 2º dia da Convenção...   \n",
       "\n",
       "                                data                 autor  \\\n",
       "0  20.ago.2024 (terça-feira) - 23h28              PODER360   \n",
       "1  20.ago.2024 (terça-feira) - 23h21  Autor não encontrado   \n",
       "2  20.ago.2024 (terça-feira) - 23h08              PODER360   \n",
       "3  20.ago.2024 (terça-feira) - 23h01              PODER360   \n",
       "4  20.ago.2024 (terça-feira) - 22h05              PODER360   \n",
       "\n",
       "                                               texto  \n",
       "0  O presidente doSTF(Supremo Tribunal Federal), ...  \n",
       "1  O candidato à Prefeitura de São PauloPablo Mar...  \n",
       "2  A vice-presidente dos Estados Unidos,Kamala Ha...  \n",
       "3  A primeira-dama, Janja Lula da Silva, particip...  \n",
       "4  A ex-secretária de imprensa da Casa Branca do ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvar em csv\n",
    "df.to_csv(\"news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
