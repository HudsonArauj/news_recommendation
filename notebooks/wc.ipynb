{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import sqlite3\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuração de email para envio de resumos\n",
    "EMAIL_HOST = os.getenv('EMAIL_HOST')\n",
    "EMAIL_PORT = os.getenv('EMAIL_PORT')\n",
    "EMAIL_USER = os.getenv('EMAIL_USER')\n",
    "EMAIL_PASSWORD = os.getenv('EMAIL_PASSWORD')\n",
    "RECIPIENT_EMAIL = os.getenv('RECIPIENT_EMAIL')\n",
    "\n",
    "# Conexão ao banco de dados SQLite para armazenar links processados\n",
    "def setup_database():\n",
    "    conn = sqlite3.connect('news.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS processed_links (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            url TEXT NOT NULL,\n",
    "            date_processed TEXT NOT NULL\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def link_already_processed(url):\n",
    "    conn = sqlite3.connect('news.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM processed_links WHERE url=?\", (url,))\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    return result is not None\n",
    "\n",
    "def store_processed_link(url):\n",
    "    conn = sqlite3.connect('news.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"INSERT INTO processed_links (url, date_processed) VALUES (?, ?)\", (url, datetime.now().strftime('%Y-%m-%d')))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Função para enviar email com o resumo das notícias\n",
    "def send_news_summary(news_data, date_str):\n",
    "    subject = f\"Resumo de Notícias - {date_str}\"\n",
    "    body = \"\\n\\n\".join(news_data)\n",
    "\n",
    "    # Configuração de mensagem\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = EMAIL_USER\n",
    "    msg['To'] = RECIPIENT_EMAIL\n",
    "    msg['Subject'] = subject\n",
    "\n",
    "    msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "    # Enviando o email\n",
    "    try:\n",
    "        with smtplib.SMTP(EMAIL_HOST, EMAIL_PORT) as server:\n",
    "            server.starttls()\n",
    "            server.login(EMAIL_USER, EMAIL_PASSWORD)\n",
    "            server.sendmail(EMAIL_USER, RECIPIENT_EMAIL, msg.as_string())\n",
    "        print(f\"Resumo de notícias enviado para {RECIPIENT_EMAIL}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao enviar email: {e}\")\n",
    "\n",
    "def get_links(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        archive_list = soup.find('ul', class_='archive-list__list')\n",
    "        li_tags = archive_list.find_all('li')\n",
    "        links = [li.find('a')['href'] for li in li_tags if li.find('a')]\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter links: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_article_info(url):\n",
    "    if link_already_processed(url):\n",
    "        print(f\"Link já processado: {url}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extração de informações com base no padrão do site\n",
    "        if url.startswith('https://www.poder360.com.br/poder-flash'):\n",
    "            title = soup.find('h1', class_='box-poder-flash__title mt-4').text.strip()\n",
    "        else:\n",
    "            title = soup.find('h1', class_='inner-page-section__title title-1').text.strip()\n",
    "\n",
    "        subtitle = soup.find('h2', class_='inner-page-section__line').text.strip()\n",
    "        date = soup.find('time').text.strip() if soup.find('time') else \"Data não encontrada\"\n",
    "        author = (soup.find('a', class_='author__name').text.strip() \n",
    "                  if soup.find('a', class_='author__name') else \"Autor não encontrado\")\n",
    "\n",
    "        # Extração do corpo do artigo\n",
    "        article_body = soup.find('div', class_='inner-page-section__text')\n",
    "        if article_body:\n",
    "            form = article_body.find('form')\n",
    "            if form:\n",
    "                form.decompose()\n",
    "\n",
    "            text_elements = []\n",
    "            for child in article_body.children:\n",
    "                if child.name == 'p':\n",
    "                    text_elements.append(child.get_text(strip=True))\n",
    "                elif child.name == 'ul':\n",
    "                    for li in child.find_all('li'):\n",
    "                        text_elements.append(li.get_text(strip=True))\n",
    "\n",
    "            text = '\\n'.join(text_elements)\n",
    "        else:\n",
    "            text = \"Texto do artigo não encontrado\"\n",
    "        \n",
    "        article_info = f\"titulo: {title}\\nsubtitulo: {subtitle}\\ndata: {date}\\nautor: {author}\\n\\ntexto: {text}\\n\\n\"\n",
    "        store_processed_link(url)\n",
    "        return article_info\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair informações do artigo: {e}\")\n",
    "        return None\n",
    "\n",
    "def my_scheduled_task(n_dias):\n",
    "    setup_database()\n",
    "    news_data = []\n",
    "\n",
    "    for i in range(1, n_dias):\n",
    "        yesterday = datetime.now() - timedelta(i)\n",
    "        yesterday_str = yesterday.strftime('%Y-%m-%d')\n",
    "        url = f'https://www.poder360.com.br/{yesterday.strftime(\"%Y/%m/%d\")}'\n",
    "        print(f\"Data e link: {url}\")\n",
    "\n",
    "        links = get_links(url)\n",
    "        filtered_links = [link for link in links if not link.startswith(\"https://www.poder360.com.br/author/\")]\n",
    "        print(f\"Links filtrados: {len(filtered_links)}\")\n",
    "\n",
    "        for link in filtered_links:\n",
    "            article_info = extract_article_info(link)\n",
    "            if article_info:\n",
    "                news_data.append(article_info)\n",
    "        \n",
    "        # Salvando notícias em arquivo JSON\n",
    "        filename = f'news_{yesterday_str}.json'\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(news_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # Enviando resumo por email\n",
    "        send_news_summary(news_data, yesterday_str)\n",
    "        print(\"Tarefa finalizada para o dia\", yesterday_str)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    my_scheduled_task(15)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
